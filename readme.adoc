= Kubernetes for Java Developer
:toc:

Deploying your Java application in a Kubernetes cluster could feel like Alice in Wonderland. You keep going down the rabbit hole and don't know how to make that ride comfortable. This repository contains source code and instructions for a Lynda course that explains how a Java application can be deployed, tested, debugged and monitored in a Kubernetes cluster. In addition, it also talks about canary deployment and deployment pipeline.

== Chapter 1: Introduction and Application

=== Introduction

**Slides**

. What is Kubernetes?
. Why Java and Kubernetes?
. Typical choices of building Java applications
. What is the typical workflow for Java + k8s?

=== Build and Test Application using Maven

We will use a simple Java application built using Spring Boot. The application publishes a REST endpoint that can be invoked at `http://{host}:{port}/hello`.

The source code is in the `app` directory.

. Show the application code in IntelliJ
.. Show `GreetingController` class
.. Explain that the code can be more complex
.. Show `pom.xml`
. Run application:

	cd app
	mvn spring-boot:run

. Test application

	curl http://localhost:8080/hello

== Chapter 2: Packaging Application using Docker

=== Introduction to Docker

**Slides**

. Why Docker?
. Explain the Docker workflow of CLI, Engine and Registry

=== Docker Image and Container

**Slides**

. Concepts to be explained:
.. Dockerfile
... FROM, ADD/COPY, CMD
... Multi-stage Dockerfile
.. Build context
.. Image tagging
.. Run a container
.. Port forward

=== Build Docker Image and Run Container

**Code**

. Create `m2.tar.gz`:

	mvn -Dmaven.repo.local=./m2 clean package
	tar cvf m2.tar.gz ./m2

. Create Docker image:
+
	docker image build -t arungupta/greeting .
+
Explain multi-stage Dockerfile.
+
. Run container:

	docker container run --name greeting -p 8080:8080 -d arungupta/greeting

. Access application:

	curl http://localhost:8080/hello

. Remove container:

	docker container rm -f greeting

=== Build Docker Image using Jib

**Slides**

The benefits of using Jib over a multi-stage Dockerfile build include:

* Don't need to install Docker or run a Docker daemon
* Don't need to write a Dockerfile or build the archive of m2 dependencies
* Much faster because it leverages Docker image layer caching

**Code**

. Explain Jib maven profile
. Create Docker image:

    mvn compile jib:build -Pjib

The above builds directly to your Docker registry. Alternatively, Jib can also build to a Docker daemon:

    mvn compile jib:dockerBuild -Pjib

=== Minimal Docker Image using Custom JRE

**Prep Work**

. Download http://download.oracle.com/otn-pub/java/jdk/11.0.1+13/90cf5d8f270a4347a95050320eef3fb7/jdk-11.0.1_linux-x64_bin.rpm[JDK 11] and `scp` to an https://aws.amazon.com/marketplace/pp/B00635Y2IW/ref=mkt_ste_ec2_lw_os_win[Amazon Linux] instance
. Install JDK 11:

	sudo yum install jdk-11.0.1_linux-x64_bin.rpm

. Clone the repo:

	git clone https://github.com/arun-gupta/lynda-k8s-for-java

. Build the application:

	cd lynda-k8s-for-java/app
	mvn package

**Code**

. Create a custom JRE for the Spring Boot application:

	cp target/app.war target/app.jar
	jlink \
		--output myjre \
		--add-modules $(jdeps --print-module-deps target/app.jar),\
		java.xml,jdk.unsupported,java.sql,java.naming,java.desktop,\
		java.management,java.security.jgss,java.instrument

. Build Docker image using this custom JRE:

	docker image build --file Dockerfile.jre -t arungupta/greeting:jre-slim .

. List the Docker images and show the difference in sizes:

	[ec2-user@ip-172-31-21-7 app]$ docker image ls | grep greeting
	arungupta/greeting   jre-slim            9eed25582f36        6 seconds ago       162MB
	arungupta/greeting   latest              1b7c061dad60        10 hours ago        490MB

. Run the container:

	docker container run -d -p 8080:8080 arungupta/greeting:jre-slim

. Access the application:

	curl http://localhost:8080/hello

== Chapter 3: Kubernetes Concepts and Getting Started

=== Introduction to Kubernetes

**Slides**

. What is Kubernetes?
. Cluster concepts
.. Control plane
.. Data plane
. Introduce kubectl

=== Kubernetes Resources

**Slides**

. Resources
.. Pod, sample config file
.. Deployment, sample config file
.. Daemonset, sample config file
.. Service, sample config file
.. Others

=== Kubernetes on Desktop

**Slides**

. Getting started
.. Minikube, Docker for Desktop
.. Cloud

**Prep Work**

Kubernetes can be easily enabled on a development machine using Docker for Mac as explained at https://docs.docker.com/docker-for-mac/#kubernetes.

. Ensure that Kubernetes is enabled in Docker for Mac

**Code**

. Show the list of contexts:

    kubectl config get-contexts

. Configure kubectl CLI for Kubernetes cluster

	kubectl config use-context docker-for-desktop

. Check the version:

	kubectl version

. Check the nodes:

	kubectl get nodes

. Check the resources:

	kubectl api-resources

=== Deploy to Kubernetes using Standalone Manifests

**Prep Work**

. Change to `manifests/standalone` directory

**Code**

. Deploy application to Kubernetes using separate manifests:

	kubectl create -f greeting-service.yaml
	kubectl create -f greeting-deployment.yaml

. Check service, deployment and pods:

	kubectl get svc
	kubectl get deployment
	kubectl get pods
	kubectl logs <pod-name>

. Access the application:

	curl http://localhost:8080/hello

. Delete service and deployment:

	kubectl delete -f greeting-service.yaml
	kubectl delete -f greeting-deployment.yaml

=== Deploy to Kubernetes using Standalone Single Manifest

**Code**

. Deploy application to Kubernetes using a single manifest:

	kubectl create -f greeting.yaml

. Check deployment, pods and service:

	kubectl get svc,deployment,pods

. Access the application:

	curl http://localhost:8080/hello

. Delete deployment and service (a different way to delete):

	kubectl delete deployment/greeting svc/greeting

=== Introduction to Helm Charts

**Slides**

. Explain what is Helm chart?
. Key concepts - client, tiller, charts
. Sample Helm chart

=== Deploy Application to Kubernetes using Helm Charts

**Prep Work**

. Change to `manifests/charts` directory

**Code**

. Install the Helm CLI:

	brew install kubernetes-helm
+
If Helm CLI is already installed then use `brew upgrade kubernetes-helm`.
+
. Check Helm version:

	helm version

. Install Helm in Kubernetes cluster:
+
	helm init
+
If Helm has already been initialized on the cluster, then you may have to upgrade Tiller:
+
	helm init --upgrade
+
. Install the Helm chart:

	helm install --name myapp myapp

. Check that the resources are running:

	kubectl get svc,deployment,pods

. Access the application:

	curl http://$(kubectl get svc/greeting \
        -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello

. Delete the Helm chart:

	helm delete --purge myapp

=== Debug Kubernetes Deployment using IntelliJ

**Code**

You can debug a Kubernetes Pod if they're running locally on your machine. (TODO: Test for remote debugging)

This was tested using Docker for Mac/Kubernetes. Use the previously deployed Helm chart.

. Install the Helm chart:

	helm install --name myapp myapp

. Show service:
+
	$ kubectl get svc
	NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
	greeting     LoadBalancer   10.99.253.180   localhost     8080:30194/TCP,5005:31755/TCP   2m
	kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP                         123d
+
Highlight the debug port is also forwarded.
+
. In IntelliJ, `Run`, `Debug`, `Remote`:
+
image::images/docker-debug1.png[]
+
. Click on `Debug`, setup a breakpoint in the class:
+
image::images/docker-debug2.png[]
+
. Access the application:

	curl http://$(kubectl get svc/myapp-greeting \
		-o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello

. Show the breakpoint hit in IntelliJ:
+
image::images/docker-debug3.png[]
+
. Click on green button to continue execution
. Invoke the application:

	curl http://locahost:8080/hello

. Delete the Helm chart:

	helm delete --purge myapp

== Chapter 4: Kubernetes Cluster on AWS

=== Introduction to Amazon EKS

**Slides**

. Introduction to Amazon EKS

=== Create an EKS Cluster

**Code**

This application will be deployed to an https://aws.amazon.com/eks/[Amazon EKS cluster]. Let's create the cluster first.

. Install http://eksctl.io/[eksctl] CLI:

	brew install weaveworks/tap/eksctl

. Check eksctl version:

	2018-12-14T18:21:05-06:00 [ℹ]  versionInfo = map[string]string{"builtAt":"2018-11-09T16:15:40Z", "gitCommit":"191474b2b0a6e6856b5f9c652c38b5f2f01bf7c9", "gitTag":"0.1.11"}

. Download AWS IAM Authenticator:
+
	curl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator
+
Include the directory where the CLI is downloaded to your `PATH`.
+
. Create EKS cluster:

	eksctl create cluster --name myeks --nodes 4 --region us-east-1
	2018-12-14T17:58:11-06:00 [ℹ]  using region us-east-1
	2018-12-14T17:58:25-06:00 [ℹ]  setting availability zones to [us-east-1c us-east-1a]
	2018-12-14T17:58:25-06:00 [ℹ]  subnets for us-east-1c - public:192.168.0.0/19 private:192.168.64.0/19
	2018-12-14T17:58:25-06:00 [ℹ]  subnets for us-east-1a - public:192.168.32.0/19 private:192.168.96.0/19
	2018-12-14T17:58:26-06:00 [ℹ]  using "ami-0a0b913ef3249b655" for nodes
	2018-12-14T17:58:26-06:00 [ℹ]  creating EKS cluster "myeks" in "us-east-1" region
	2018-12-14T17:58:26-06:00 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
	2018-12-14T17:58:26-06:00 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=myeks'
	2018-12-14T17:58:26-06:00 [ℹ]  creating cluster stack "eksctl-myeks-cluster"
	2018-12-14T18:10:34-06:00 [ℹ]  creating nodegroup stack "eksctl-myeks-nodegroup-0"
	2018-12-14T18:14:29-06:00 [✔]  all EKS cluster resource for "myeks" had been created
	2018-12-14T18:14:29-06:00 [✔]  saved kubeconfig as "/Users/argu/.kube/config"
	2018-12-14T18:14:35-06:00 [ℹ]  the cluster has 1 nodes
	2018-12-14T18:14:35-06:00 [ℹ]  node "ip-192-168-28-25.ec2.internal" is not ready
	2018-12-14T18:14:35-06:00 [ℹ]  waiting for at least 4 nodes to become ready
	2018-12-14T18:15:01-06:00 [ℹ]  the cluster has 4 nodes
	2018-12-14T18:15:01-06:00 [ℹ]  node "ip-192-168-28-25.ec2.internal" is ready
	2018-12-14T18:15:01-06:00 [ℹ]  node "ip-192-168-3-103.ec2.internal" is ready
	2018-12-14T18:15:01-06:00 [ℹ]  node "ip-192-168-44-70.ec2.internal" is ready
	2018-12-14T18:15:01-06:00 [ℹ]  node "ip-192-168-59-35.ec2.internal" is ready
	2018-12-14T18:15:06-06:00 [ℹ]  kubectl command should work with "/Users/argu/.kube/config", try 'kubectl get nodes'
	2018-12-14T18:15:06-06:00 [✔]  EKS cluster "myeks" in "us-east-1" region is ready

. Check the nodes:

	kubectl get nodes
	NAME                            STATUS   ROLES    AGE   VERSION
	ip-192-168-28-25.ec2.internal   Ready    <none>   16m   v1.10.3
	ip-192-168-3-103.ec2.internal   Ready    <none>   15m   v1.10.3
	ip-192-168-44-70.ec2.internal   Ready    <none>   16m   v1.10.3
	ip-192-168-59-35.ec2.internal   Ready    <none>   15m   v1.10.3

. Get the list of configs:
+
	kubectl config get-contexts
	CURRENT   NAME                               CLUSTER                       AUTHINFO                           NAMESPACE
	*         arun@myeks.us-east-1.eksctl.io     myeks.us-east-1.eksctl.io     arun@myeks.us-east-1.eksctl.io     
	          docker-for-desktop                 docker-for-desktop-cluster    docker-for-desktop   
+
`*` indicates that kubectl is now configured to talk to the newly created cluster.

=== Migrate Application to Kubernetes Cluster on AWS

**Code**

. Explicitly set the context:

    kubectl config use-context arun@myeks.us-east-1.eksctl.io

. Install Helm in EKS:

	kubectl -n kube-system create sa tiller
	kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
	helm init --service-account tiller

. Check the list of pods:

	kubectl get pods -n kube-system
	NAME                            READY   STATUS    RESTARTS   AGE
	aws-node-6h4r9                  1/1     Running   1          16m
	aws-node-7rwkw                  1/1     Running   1          16m
	aws-node-k9g6s                  1/1     Running   0          16m
	aws-node-t6k6v                  1/1     Running   1          16m
	kube-dns-64b69465b4-vpxjq       3/3     Running   0          23m
	kube-proxy-bnkj6                1/1     Running   0          16m
	kube-proxy-bqths                1/1     Running   0          16m
	kube-proxy-m7ctf                1/1     Running   0          16m
	kube-proxy-tszj5                1/1     Running   0          16m
	tiller-deploy-895d57dd9-zt2s2   1/1     Running   0          7s

. Redeploy the application:

	helm install --name myapp manifests/charts/myapp

. Get the service:
+
	kubectl get svc
	NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                         AGE
	greeting     LoadBalancer   10.100.12.193   a2d4c846f000111e9be5e0a988475aff-1529459134.us-east-1.elb.amazonaws.com   8080:32676/TCP,5005:32536/TCP   20s
	kubernetes   ClusterIP      10.100.0.1      <none>                                                                    443/TCP                         26m
+
It shows the port `8080` and `5005` are published and an Elastic Load Balancer is provisioned. It takes about three minutes for the load balancer to be ready.
+
. Access the application:

	curl http://$(kubectl get svc/greeting \
		-o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello

. Delete the application:

	helm delete --purge myapp

=== Backup and Restore Cluster using Heptio Ark

TODO

== Chapter 5: Istio Service Mesh

=== Introduction to Istio Service Mesh

**Slides**

. What is service mesh?
. Envoy
. What is Istio?
. Istio components - Pilot, Mixer, Citadel
. Istio resources
.. Traffic shifting
.. Canary deployment
.. Distributed Tracing
.. Telemetry

=== Install and Configure Istio on EKS

More details at https://aws.amazon.com/blogs/opensource/getting-started-istio-eks/[Getting Started with Istio on Amazon EKS].

**Code**

. Download Istio:

	curl -L https://git.io/getLatestIstio | sh -
	cd istio-1.*

. Include `istio-1.*/bin` directory in `PATH`
. Install Istio on Amazon EKS:

	helm install \
		--wait \
		--name istio \
		--namespace istio-system \
		install/kubernetes/helm/istio \
		--set tracing.enabled=true \
		--set grafana.enabled=true

. Verify:
+
	kubectl get pods -n istio-system
	NAME                                        READY     STATUS    RESTARTS   AGE
	grafana-75485f89b9-n4skw                    1/1       Running   0          1m
	istio-citadel-84fb7985bf-bv2tm              1/1       Running   0          1m
	istio-egressgateway-bd9fb967d-qls6z         1/1       Running   0          1m
	istio-galley-655c4f9ccd-nblsb               1/1       Running   0          1m
	istio-ingressgateway-688865c5f7-xmm46       1/1       Running   0          1m
	istio-pilot-6cd69dc444-5j8kv                2/2       Running   0          1m
	istio-policy-6b9f4697d-fpr9g                2/2       Running   0          1m
	istio-statsd-prom-bridge-7f44bb5ddb-rlt77   1/1       Running   0          1m
	istio-telemetry-6b5579595f-f7bd7            2/2       Running   0          1m
	istio-tracing-ff94688bb-47zlc               1/1       Running   0          1m
	prometheus-84bd4b9796-lrkkv                 1/1       Running   0          1m
+
Check that both Tracing and Grafana add-ons are enabled.

=== Deploy Istio-enabled Application

**Prep Work**

Change to `manifests/charts` directory

. Enable `default` namespace injection:

	kubectl label namespace default istio-injection=enabled

. Talk about `istioctl` in case `default` namespace injection cannot be enabled:

	kubectl apply -f $(istioctl kube-inject -f manifest.yaml)

. TODO: How does istioctl work with Helm?
. Deploy the application:

	helm install --name myapp myapp

. Check pods and note that it has two containers (one for the application and one for the sidecar):

	kubectl get pods -l app=greeting
	NAME                       READY     STATUS    RESTARTS   AGE
	greeting-d4f55c7ff-6gz8b   2/2       Running   0          5s

. Get the list of containers in the pod:

	kubectl get pods -l app=greeting -o jsonpath={.items[*].spec.containers[*].name}
	greeting istio-proxy

. Get response:
+
  curl http://$(kubectl get svc/greeting \
  	-o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello
+
It takes about three minutes for the ELB to be ready to receive requests.

=== Round Robin between Two Deployments

. Deploy application with two versions of `greeting`, one that returns `Hello` and another that returns `Howdy`:

  helm delete myapp --purge
  helm install --name myapp myapp-hello-howdy

. Check the list of pods:

	kubectl get pods -l app=greeting
	NAME                              READY     STATUS    RESTARTS   AGE
	greeting-hello-69cc7684d-7g4bx    2/2       Running   0          1m
	greeting-howdy-788b5d4b44-g7pml   2/2       Running   0          1m

. Access application multipe times to see alternating response from the two deployments:

  for i in {1..10}
  do
  	curl -q http://$(kubectl get svc/greeting -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello
  	echo
  done

=== Traffic Shifting using Istio

**Prep Work**

Change to `manifests` directory

**Code**
  
. Setup an Istio rule to split traffic between 75% to `Hello` and 25% to `Howdy` version of the `greeting` service:

	kubectl apply -f standalone/greeting-rule-75-25.yaml

. Check created manfiests:

	kubectl get virtualservice,destinationrule

. Invoke the service again to see the traffic split between two services:

  for i in {1..50}
  do
  	curl -q http://$(kubectl get svc/greeting-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello
  	echo
  done

=== Canary Deployment using Istio

. Setup an Istio rule to divert 10% traffic to canary:

  kubectl delete -f standalone/greeting-rule-75-25.yaml
  kubectl apply -f standalone/greeting-canary.yaml

. Access application multipe times to see ~10% greeting messages with `Howdy`:

  for i in {1..50}
  do
  	curl -q http://$(kubectl get svc/greeting-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello
  	echo
  done

=== Telemetry using Istio

**Code**

. By default, Grafana is disabled. `--set grafana.enabled=true` was used during Istio installation to ensure Grafana was enabled. Alternatively, the Grafana add-on can be installed as:

	kubectl apply -f install/kubernetes/addons/grafana.yaml

. Verify:

	kubectl get pods -l app=grafana -n istio-system
	NAME                       READY     STATUS    RESTARTS   AGE
	grafana-75485f89b9-n4skw   1/1       Running   0          10m

. Forward Istio dashboard using Grafana UI:

	kubectl -n istio-system \
		port-forward $(kubectl -n istio-system \
			get pod -l app=grafana \
			-o jsonpath='{.items[0].metadata.name}') 3000:3000 &

. View Istio dashboard http://localhost:3000/d/1/istio-dashboard?
. Invoke the endpoint a few times:

	for i in {1..50}
	do
		curl -q http://$(kubectl get svc/greeting-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello
		echo
	done

. Show the Grafana dashboard:
+
image::images/istio-dashboard.png[]

== Chapter 6: Deployment Pipelines

=== Introduction to Deployment Pipelines

**Slides**

. What is deployment pipeline?
. What is Skaffold?
.. Key benefits
.. Workflow
. Code Pipeline + Code Commit
.. Key benefits
.. Workflow
. JenkinsX
.. Key benefits
.. Workflow

=== Skaffold

**Code**

https://github.com/GoogleContainerTools/skaffold[Skaffold] is a command line utility that facilitates continuous development for Kubernetes applications. With Skaffold, you can iterate on your application source code locally then deploy it to a remote Kubernetes cluster.

. Check context:

	kubectl config get-contexts
	CURRENT   NAME                               CLUSTER                       AUTHINFO                           NAMESPACE
	*         arun@myeks.us-east-1.eksctl.io     myeks.us-east-1.eksctl.io     arun@myeks.us-east-1.eksctl.io     
	          docker-for-desktop                 docker-for-desktop-cluster    docker-for-desktop

. Change to use local Kubernetes cluster:

	kubectl config use-context docker-for-desktop

. Download Skaffold:

	curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-darwin-amd64 \
		&& chmod +x skaffold

. Open http://localhost:8080/hello in browser. This will show that the page is not available.
. Run Skaffold in the application directory:

    cd app
    skaffold dev

. Refresh the page in browser to see the output.

=== Code Pipeline

**Code**

https://eksworkshop.com/codepipeline/codepipeline/

Use the following command to access the service:

	curl http://$(kubectl get svc/hello-k8s -o jsonpath='{.status.loadBalancer.ingress[0].hostname}’)

=== Deployment Pipeline using JenkinsX

TBD
